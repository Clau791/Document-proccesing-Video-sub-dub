"""
Validator LLM pentru traduceri - foloseÈ™te Ollama cu Gemma3 sau Mistral
"""

import requests
import json
import time
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import concurrent.futures
from tqdm import tqdm

@dataclass
class ValidationResult:
    original_text: str
    initial_translation: str
    validated_translation: str
    confidence_score: float
    model_used: str
    validation_time: float

class LLMTranslationValidator:
    """Validator È™i Ã®mbunÄƒtÄƒÈ›itor de traduceri folosind LLM local"""
    
    def __init__(
        self,
        ollama_url: str = "http://86.126.134.77:11434",
        primary_model: str = "gemma3:27b",
        fallback_model: str = "mistral:Q4_K_M",
        max_retries: int = 3
    ):
        self.base_url = f"{ollama_url}/api/generate"
        self.primary_model = primary_model
        self.fallback_model = fallback_model
        self.max_retries = max_retries
        self.session = requests.Session()
        
        # Cache pentru traduceri validate
        self.cache = {}
        
        print(f"ğŸ¤– LLM Validator iniÈ›ializat")
        print(f"   Primary: {primary_model}")
        print(f"   Fallback: {fallback_model}")
        print(f"   Server: {ollama_url}")
        
        # Test conexiune
        self._test_connection()
    
    def _test_connection(self):
        """TesteazÄƒ conexiunea la Ollama"""
        try:
            test_payload = {
                "model": self.primary_model,
                "prompt": "Salut",
                "stream": False,
                "num_predict": 5
            }
            
            response = self.session.post(
                self.base_url,
                json=test_payload,
                timeout=10
            )
            
            if response.status_code == 200:
                print("âœ… Conexiune LLM OK")
            else:
                print(f"âš ï¸ Status code: {response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ Nu se poate conecta la LLM: {e}")
            print("   Validarea va funcÈ›iona Ã®n modul offline")
    
    def validate_translation(
        self,
        original_text: str,
        translated_text: str,
        source_lang: str,
        target_lang: str,
        context: str = "",
        use_streaming: bool = True
    ) -> ValidationResult:
        """ValideazÄƒ È™i Ã®mbunÄƒtÄƒÈ›eÈ™te o traducere"""
        
        start_time = time.time()
        
        # Check cache
        cache_key = (original_text, source_lang, target_lang)
        if cache_key in self.cache:
            cached = self.cache[cache_key]
            cached.validation_time = 0.01  # From cache
            return cached
        
        # ConstruieÈ™te prompt specializat
        prompt = self._build_validation_prompt(
            original_text,
            translated_text,
            source_lang,
            target_lang,
            context
        )
        
        # ÃncearcÄƒ cu modelul principal
        validated_text = self._call_llm(
            prompt,
            self.primary_model,
            use_streaming
        )
        
        # Fallback dacÄƒ e necesar
        if not validated_text or validated_text == translated_text:
            validated_text = self._call_llm(
                prompt,
                self.fallback_model,
                use_streaming
            )
            model_used = self.fallback_model
        else:
            model_used = self.primary_model
        
        # DacÄƒ tot nu avem rezultat valid, pÄƒstrÄƒm traducerea originalÄƒ
        if not validated_text:
            validated_text = translated_text
            confidence = 0.5
        else:
            # CalculeazÄƒ scor de Ã®ncredere
            confidence = self._calculate_confidence(
                original_text,
                translated_text,
                validated_text
            )
        
        result = ValidationResult(
            original_text=original_text,
            initial_translation=translated_text,
            validated_translation=validated_text,
            confidence_score=confidence,
            model_used=model_used,
            validation_time=time.time() - start_time
        )
        
        # Cache rezultatul
        self.cache[cache_key] = result
        
        return result
    
    def validate_batch(
        self,
        segments: List[Dict],
        source_lang: str,
        target_lang: str,
        batch_size: int = 5,
        parallel: bool = False
    ) -> List[Dict]:
        """ValideazÄƒ un batch de segmente de subtitrare"""
        
        print(f"\nğŸ” Validare traduceri cu LLM...")
        print(f"   Segmente: {len(segments)}")
        print(f"   LimbÄƒ: {source_lang} â†’ {target_lang}")
        
        validated_segments = []
        
        if parallel and len(segments) > 10:
            # Procesare paralelÄƒ pentru multe segmente
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = []
                
                for seg in segments:
                    future = executor.submit(
                        self.validate_translation,
                        seg.get('original_text', ''),
                        seg.get('text', ''),
                        source_lang,
                        target_lang,
                        use_streaming=False
                    )
                    futures.append((seg, future))
                
                for seg, future in tqdm(futures, desc="Validare LLM"):
                    try:
                        result = future.result(timeout=30)
                        seg['text'] = result.validated_translation
                        seg['llm_confidence'] = result.confidence_score
                        seg['llm_model'] = result.model_used
                        validated_segments.append(seg)
                    except:
                        validated_segments.append(seg)
        else:
            # Procesare secvenÈ›ialÄƒ
            for seg in tqdm(segments, desc="Validare LLM"):
                original = seg.get('original_text', '')
                translated = seg.get('text', '')
                
                if not original or not translated:
                    validated_segments.append(seg)
                    continue
                
                result = self.validate_translation(
                    original,
                    translated,
                    source_lang,
                    target_lang,
                    use_streaming=False
                )
                
                seg['text'] = result.validated_translation
                seg['llm_confidence'] = result.confidence_score
                seg['llm_model'] = result.model_used
                
                validated_segments.append(seg)
        
        # Statistici
        avg_confidence = sum(
            s.get('llm_confidence', 0) for s in validated_segments
        ) / len(validated_segments)
        
        print(f"âœ… Validare completÄƒ")
        print(f"   Ãncredere medie: {avg_confidence:.1%}")
        
        return validated_segments
    
    def _build_validation_prompt(
        self,
        original: str,
        translation: str,
        source_lang: str,
        target_lang: str,
        context: str = ""
    ) -> str:
        """ConstruieÈ™te prompt pentru validare"""
        
        lang_names = {
            'ro': 'romÃ¢nÄƒ',
            'en': 'englezÄƒ',
            'zh': 'chinezÄƒ',
            'ja': 'japonezÄƒ',
            'ru': 'rusÄƒ'
        }
        
        src_name = lang_names.get(source_lang, source_lang)
        tgt_name = lang_names.get(target_lang, target_lang)
        
        # Prompt optimizat pentru validare È™i corecÈ›ie
        prompt = f"""EÈ™ti un expert traducÄƒtor profesionist. ValideazÄƒ È™i Ã®mbunÄƒtÄƒÈ›eÈ™te urmÄƒtoarea traducere.

TEXT ORIGINAL ({src_name}):
{original}

TRADUCERE INIÈšIALÄ‚ ({tgt_name}):
{translation}

INSTRUCÈšIUNI:
1. VerificÄƒ dacÄƒ traducerea este corectÄƒ È™i completÄƒ
2. PÄƒstreazÄƒ sensul È™i tonul originalului
3. CorecteazÄƒ orice erori gramaticale sau de exprimare
4. AsigurÄƒ-te cÄƒ traducerea sunÄƒ natural Ã®n {tgt_name}
5. Pentru subtitrÄƒri, pÄƒstreazÄƒ textul concis È™i clar

RÄ‚SPUNDE DOAR CU TRADUCEREA CORECTATÄ‚/VALIDATÄ‚, FÄ‚RÄ‚ EXPLICAÈšII:"""
        
        if context:
            prompt += f"\n\nCONTEXT: {context}"
        
        return prompt
    
    def _call_llm(
        self,
        prompt: str,
        model: str,
        use_streaming: bool = True
    ) -> Optional[str]:
        """ApeleazÄƒ LLM-ul pentru validare"""
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": use_streaming,
            "temperature": 0.3,  # Mai deterministÄƒ pentru traduceri
            "top_p": 0.9,
            "num_predict": 256,  # LimitÄƒ rezonabilÄƒ
            "keep_alive": "30m"
        }
        
        try:
            if use_streaming:
                # Streaming pentru feedback real-time
                response = self.session.post(
                    self.base_url,
                    json=payload,
                    stream=True,
                    timeout=60
                )
                
                if response.status_code != 200:
                    return None
                
                full_text = ""
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if 'response' in data:
                                full_text += data['response']
                            
                            if data.get('done', False):
                                break
                                
                        except json.JSONDecodeError:
                            continue
                
                return full_text.strip()
            
            else:
                # Non-streaming pentru batch processing
                payload['stream'] = False
                response = self.session.post(
                    self.base_url,
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get('response', '').strip()
                
        except requests.exceptions.Timeout:
            print(f"â±ï¸ Timeout pentru {model}")
        except Exception as e:
            print(f"âš ï¸ Eroare LLM {model}: {e}")
        
        return None
    
    def _calculate_confidence(
        self,
        original: str,
        initial: str,
        validated: str
    ) -> float:
        """CalculeazÄƒ scor de Ã®ncredere pentru validare"""
        
        # VerificÄƒri de bazÄƒ
        if not validated or validated == initial:
            return 0.7  # Traducerea iniÈ›ialÄƒ pÄƒstratÄƒ
        
        # VerificÄƒ lungimea
        len_ratio = len(validated) / max(len(original), 1)
        if 0.5 < len_ratio < 2.0:
            length_score = 1.0
        else:
            length_score = 0.5
        
        # VerificÄƒ cÄƒ nu e text halucinat
        if len(validated) > len(original) * 3:
            return 0.3  # Probabil halucinaÈ›ie
        
        # Scor final
        confidence = min(0.95, length_score * 0.9)
        
        return confidence
    
    def double_validation(
        self,
        segments: List[Dict],
        source_lang: str,
        target_lang: str
    ) -> List[Dict]:
        """Validare dublÄƒ cu ambele modele pentru acurateÈ›e maximÄƒ"""
        
        print("\nğŸ”ğŸ” Validare dublÄƒ activatÄƒ")
        
        validated_segments = []
        
        for seg in tqdm(segments, desc="Validare dublÄƒ"):
            original = seg.get('original_text', '')
            translated = seg.get('text', '')
            
            if not original or not translated:
                validated_segments.append(seg)
                continue
            
            # Validare cu modelul principal
            result1 = self.validate_translation(
                original, translated,
                source_lang, target_lang,
                use_streaming=False
            )
            
            # Validare cu modelul de backup
            prompt = self._build_validation_prompt(
                original, translated,
                source_lang, target_lang
            )
            
            text2 = self._call_llm(prompt, self.fallback_model, False)
            
            # ComparÄƒ rezultatele
            if result1.validated_translation == text2:
                # Ambele modele sunt de acord
                seg['text'] = result1.validated_translation
                seg['llm_confidence'] = 0.95
                seg['validation_type'] = 'double_match'
            else:
                # Alege varianta mai probabilÄƒ
                if result1.confidence_score > 0.7:
                    seg['text'] = result1.validated_translation
                else:
                    seg['text'] = text2 if text2 else translated
                
                seg['llm_confidence'] = 0.7
                seg['validation_type'] = 'double_mismatch'
            
            validated_segments.append(seg)
        
        return validated_segments